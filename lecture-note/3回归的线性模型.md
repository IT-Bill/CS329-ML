[toc]

## 0. 基函数 (basis function) 模型

- 参数的线性函数
- 输入变量的非线性函数

## 1. 线性基函数模型

$$
\begin{align}
y(\boldsymbol x, \boldsymbol w) &= w_0 + \sum^{M-1}_{j=1} w_j \phi_j(\boldsymbol x)
\\&= \sum^{M-1}_{j=0} w_j \phi_j(\boldsymbol x)
\\&= \boldsymbol w^T \boldsymbol \phi(\boldsymbol x)
\end{align}
$$

where $\boldsymbol w = (w_0, \dots, w_{M-1})^T$, $\boldsymbol \phi = (\phi_0, \dots, \phi_{M-1})$, $\phi_0(\boldsymbol x) = 1$.

$w_0$是偏置参数 (bias parameter).

#### 1.1 最大似然与最小平方

目标变量$t$由确定的函数$y(\boldsymbol x, \boldsymbol w)$给出，这个函数被附加了高斯噪声$\epsilon \sim \mathcal N(0, \beta)$
$$
t = y(\boldsymbol x, \boldsymbol w) + \epsilon
$$
因此
$$
p(t| \boldsymbol x, \boldsymbol w, \beta) = \mathcal N(t | y(\boldsymbol x, \boldsymbol w), \beta^{-1})
$$
对于新的$\boldsymbol x$​, 最优的预测由目标变量的条件均值给出
$$
\mathbb E[t| \boldsymbol x] = \int tp(t| \boldsymbol x) dt = y(\boldsymbol x, \boldsymbol w)
$$
考虑一个输入数据集$\boldsymbol X = \{ \boldsymbol x_1, \dots, \boldsymbol x_N \}$，对应的目标值是$t_1, \dots, t_N$ (可以组成列向量$\boldsymbol t$)。

似然函数表达式，它是可调节参数$\boldsymbol w$和$\beta$的函数
$$
p(\boldsymbol t|\boldsymbol X, \boldsymbol w, \beta) = \prod^N_{n=1} \mathcal N(t_n|\boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n), \beta^{-1})
$$
在监督学习问题中，$\boldsymbol x$总会出现在条件变量的位置上，所以在诸如$p(t| \boldsymbol x, \boldsymbol w, \beta)$的表达式中不显式地写出$\boldsymbol x$。

取对数函数的对数，使用一元高斯分布的标准形式
$$
\begin{align}
\ln p(\boldsymbol t| \boldsymbol w, \beta) &= \ln\left\{\frac{1}{\sqrt{2\pi} \sqrt{\beta^{-1}}} \exp\{ \frac{E_D(\boldsymbol w)}{\beta^{-1}} \} \right\}
\\&=
\frac{N}{2} \ln \beta - \frac{N}{2} \ln(2\pi) - \beta E_D(\boldsymbol w)
\end{align}
$$

其中平方和误差函数的定义为
$$
E_D(\boldsymbol w) = \frac{1}{2} \sum^N_{n=1}\{ t_n - \boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n) \}^2
$$

线性模型的似然函数的最大化等价于平方和误差函数$E_D(\boldsymbol w)$的最小化，
$$
\nabla_\boldsymbol w \ln p(\boldsymbol t| \boldsymbol w, \beta) = \beta \sum^N_{n=1} \{ t_n - \boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n) \} \boldsymbol \phi(\boldsymbol x_n)^T
$$

> 上述式子的计算过程：
>
> 对参数$\boldsymbol w$求$E_D(\boldsymbol w)$的梯度，我们对每个$w_j$分量求导数
> $$
> \begin{align}
> \frac{\partial}{\partial w_j} E_D(\boldsymbol w) &= \frac{\partial}{\partial w_j} \frac{1}{2} \sum^N_{n=1}\{ t_n - \boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n) \}^2
> \\&=
> \sum^N_{n=1}\{ t_n - \boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n) \} (-\phi_j(\boldsymbol x_n))
> 
> \end{align}
> $$
> 其中$\phi_j(\boldsymbol x_n)$代表特征向量$\boldsymbol \phi(\boldsymbol x_n)$中的第$j$个元素。
>
> 而
> $$
> \frac{\partial}{\partial \boldsymbol w} E_D(\boldsymbol w) = 
> \left[ 
> \frac{\partial}{\partial w_1} E_D(\boldsymbol w) \quad 
> \frac{\partial}{\partial w_2} E_D(\boldsymbol w) \quad
> \cdots \quad
> \frac{\partial}{\partial w_N} E_D(\boldsymbol w)
> \right]
> $$

令梯度等于$\boldsymbol 0$，可得
$$
\boldsymbol 0 = \sum^N_{n=1}t_n \boldsymbol \phi(\boldsymbol x_n)^T - \boldsymbol w^T \left( \sum^N_{n=1} \boldsymbol \phi(\boldsymbol x_n) \boldsymbol \phi(\boldsymbol x_n)^T \right)
$$
求解$\boldsymbol w$，可得
$$
\boldsymbol w_{ML} = (\boldsymbol \Phi^T \boldsymbol \Phi)^{-1} \boldsymbol \Phi^T \boldsymbol t
$$

> $$
> \begin{align*}
> \sum^N_{n=1}t_n \boldsymbol \phi(\boldsymbol x_n)^T 
> &= t_1 [\phi_0(x_1) \;\; \phi_1(x_1) \; \cdots \; \phi_{M-1}(x_1)] 
> \\&+ t_2 [\phi_0(x_2) \;\; \phi_1(x_2) \; \cdots \; \phi_{M-1}(x_2)]
> \\&+ \cdots
> \\&+ t_N [\phi_0(x_N) \;\; \phi_1(x_N) \; \cdots \; \phi_{M-1}(x_N)]
> \\&= 
> 
> \begin{bmatrix}
> \phi_0(x_1) & \phi_1(x_1) & \cdots & \phi_{M-1}(x_1) \\
> \phi_0(x_2) & \phi_1(x_2) & \cdots & \phi_{M-1}(x_2) \\
> \vdots & \vdots & \ddots & \vdots \\
> \phi_0(x_N) & \phi_1(x_N) & \cdots & \phi_{M-1}(x_N)
> \end{bmatrix}
> \begin{bmatrix}
> t_1 & t_2 & \cdots & t_N
> \end{bmatrix}
> 
> \end{align*}
> $$
>
> 
>

这被称为最小平方问题的**规范方程** (normal equation)，$\boldsymbol \Phi$是$N \times M$的矩阵，被称为**设计矩阵** (design matrix)，元素为$\Phi_{nj} = \phi_j(\boldsymbol x_n)$
$$
\boldsymbol \Phi = 
\begin{bmatrix}
\phi_0(x_1) & \phi_1(x_1) & \cdots & \phi_{M-1}(x_1) \\
\phi_0(x_2) & \phi_1(x_2) & \cdots & \phi_{M-1}(x_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(x_N) & \phi_1(x_N) & \cdots & \phi_{M-1}(x_N)
\end{bmatrix}
$$
量
$$
\boldsymbol \Phi^+ \equiv (\boldsymbol \Phi^T \boldsymbol \Phi)^{-1} \boldsymbol \Phi^T
$$
被称为矩阵$\boldsymbol \Phi$的**伪逆矩阵** (pseudo-inverse matrix)。

对于偏置参数$w_0$
$$
E_D(\boldsymbol w) = \frac{1}{2} \sum^{N}_{n=1}\{ t_n - w_0 - \sum^{M-1}_{j=1} w_j \phi_j(\boldsymbol x_n) \}^2
$$
令关于$w_0$的导数等于0，可得
$$
w_0 = \overline t - \sum^{M-1}_{j=1} w_j \overline\phi_j
$$
其中
$$
\overline t = \frac{1}{N} \sum^{N}_{n=1}t_n, \quad \overline \phi_j = \frac{1}{N} \sum^{N}_{n=1} \phi_j(\boldsymbol x_n)
$$
因此偏置$w_0$补偿了目标值的平均值（在训练集上的）与基函数的值的平均值的加权求和之间的差。

关于噪声精度参数$\beta$最大化似然函数

$$
\frac{1}{\beta_{ML}} = \frac{1}{N} \sum^N_{n=1} \{ t_n - \boldsymbol w^T_{ML} \boldsymbol \phi(\boldsymbol x_n) \}^2
$$

> $$
> \nabla_\beta \ln p(\boldsymbol t| \boldsymbol w, \beta) = 0
> $$

噪声精度的倒数由⽬标值在回归函数周围的残留⽅差给出。

## 2. 偏置-方差 (bias-variance) 分解

$$
h(\boldsymbol x) = \mathbb E[t|\boldsymbol x] = \int tp(t|\boldsymbol x) dt
$$

$h(\boldsymbol x)$为条件期望，即给定输入$\boldsymbol x$时目标变量$t$的期望值。

**平方损失函数的期望**可以写成
$$
\mathbb E[L] = \int \{y(\boldsymbol x) - h(\boldsymbol x)\}^2 p(\boldsymbol x) d\boldsymbol x + \int\int \{h(\boldsymbol x)-t \}^2(\boldsymbol x, t) d\boldsymbol x dt
$$


第二项与$y(\boldsymbol x)$无关，由数据本身的噪声造成，表示期望损失能达到的最小值。

需要找一个$y(\boldsymbol x)$的解，使得第一项最小。

如果使用由参数向量$\boldsymbol w$控制的函数$y(\boldsymbol x, \boldsymbol w)$对$h(\boldsymbol x)$进行建模，则模型的不确定性由$\boldsymbol w$的后验分布来表示。

对于任意给定的数据集$\mathcal D$，得到预测函数$y(\boldsymbol x; \mathcal D)$。不同的数据集给出不同的函数，算法的表现就可以通过平均值计算。

考虑平方损失函数的期望第一项
$$
\begin{align*}
\{y(\boldsymbol x; \mathcal D) - h(\boldsymbol x)\}^2 
&= 
\{y(\boldsymbol x; \mathcal D) - \mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)] + \mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)] - h(\boldsymbol x)\}^2
\\&= 
\{y(\boldsymbol x; \mathcal D) - \mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)]\}^2 + \{\mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)] - h(\boldsymbol x)\}^2 
\\&+
\{y(\boldsymbol x; \mathcal D) - \mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)]\}\{\mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)] - h(\boldsymbol x)\}
\end{align*}
$$
关于$\mathcal D$求期望，最后一项等于0，可得
$$
\mathbb E_{\mathcal D}[\{y(\boldsymbol x; \mathcal D) - h(\boldsymbol x)\}^2 ] =
\underbrace{\{\mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)] - h(\boldsymbol x)\}^2}_{\text{bias}^2} + 
\underbrace{\mathbb E_{\mathcal D}[\{y(\boldsymbol x; \mathcal D) - \mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)]\}^2]}_{\text{variance}}
$$
`bias`：$\{\mathbb E_{\mathcal D}[y(\boldsymbol x;\mathcal D)] - h(\boldsymbol x)\}^2$表示**所有**数据集的平均预测与预期的回归函数之间的差异，是一个常数，取期望后不变。

`variance`: 度量了对于**单独**的数据集，模型所给出的解在平均值附近波动的情况，因此也就度量了函数$y(\boldsymbol x; \mathcal D)$对于特定的数据集的选择的敏感程度。



## 3. 贝叶斯线性回归



### 3.1 参数分布

#### 3.1.1 定义

引入模型参数$\boldsymbol w$的先验概率分布：

- 把噪声精度$\beta$作为已知常数

- 由公式
  $$
  p(\boldsymbol t|\boldsymbol X, \boldsymbol w, \beta) = \prod^N_{n=1} \mathcal N(t_n|\boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n), \beta^{-1})
  $$
  定义的似然函数是高斯分布

- 对应的共轭先验是高斯分布，形式为
  $$
  p(\boldsymbol w) = \mathcal N(\boldsymbol w|\boldsymbol m_0, \boldsymbol S_0)
  $$

> 关于共轭先验：
>
> - 共轭先验（Conjugate prior）是贝叶斯统计中的一个概念，它涉及到选择先验分布的一种特定方式。
> - 在贝叶斯统计中，通常有
>   - 似然函数（likelihood function），它基于数据来描述参数的可能性
>   - 先验分布（prior distribution），它在看到数据之前描述了参数的信念或知识
>   - 后验分布（posterior distribution），它结合了似然函数和先验分布的信息
> - 如果先验分布和似然函数是共轭的，那么后验分布将与先验分布**属于同一族**分布。

后验分布正比于likelihood和prior的乘积，也是高斯分布，形式为
$$
p(\boldsymbol w|\boldsymbol t) = \mathcal N(\boldsymbol w|\boldsymbol m_N, \boldsymbol S_N)
$$
其中
$$
\begin{align}
\boldsymbol m_N &= \boldsymbol S_N(\boldsymbol S_0^{-1}\boldsymbol m_0 + \beta\boldsymbol \Phi^T \boldsymbol t) \\
\boldsymbol S_N^{-1} &= \boldsymbol S_0^{-1} + \beta \boldsymbol \Phi^T \boldsymbol \Phi
\end{align}
$$

> $$
> p(\boldsymbol w|\boldsymbol t) \propto p(\boldsymbol t|\boldsymbol w) \times p(\boldsymbol w)
> $$
>
> 相同次数的比较即可

#### 3.3.2 简化

为了简化起见，考虑高斯先验的一个特定形式：

- 零均值
- 各向同性

这个分布由一个精度参数$\alpha$控制
$$
p(\boldsymbol w| \alpha) = \mathcal N(\boldsymbol w| \boldsymbol 0, \alpha^{-1}\boldsymbol I)
$$
对应的后验分布为
$$
\begin{align}
\boldsymbol m_N &= \beta \boldsymbol S_N \boldsymbol \Phi^T \boldsymbol t \\
\boldsymbol S_N^{-1} &= \alpha \boldsymbol I + \beta \boldsymbol \Phi^T \boldsymbol \Phi
\end{align}
$$


### 3.2 预测分布

对于新的$\boldsymbol x$预测出$t$的值。

预测分布定义为
$$
p(t|\boldsymbol t, \alpha, \beta) = \int p(t|\boldsymbol w, \beta)p(\boldsymbol w|\boldsymbol t, \alpha, \beta) d\boldsymbol w
$$
其中$\boldsymbol t$是训练数据的目标变量的值组成的向量，且上式省略了条件概率中出现的输入向量。

这个公式是在说：给定输入$\boldsymbol x$和先验参数$\alpha, \beta$，新数据点$t$的概率分布可以通过对所有可能的参数$\boldsymbol w$的后验分布$p(\boldsymbol w|\boldsymbol t, \alpha, \beta)$加权其似然函数$p(t|\boldsymbol w, \beta)$来计算。换句话说，你需要对所有可能的参数值，根据它们的后验概率，计算新数据点的似然，并将这些似然加权平均。

在给定新的$\boldsymbol x$的情况下，预测分布的形式为
$$
p(t|\boldsymbol x, \boldsymbol t, \alpha, \beta) = \mathcal N(t|\boldsymbol m^T_N \boldsymbol \phi(\boldsymbol x), \sigma^2_N(\boldsymbol x))
$$
其中预测分布的方差为
$$
\sigma^2_N(\boldsymbol x) = \frac{1}{\beta} + \boldsymbol \phi(\boldsymbol x)^T \boldsymbol S_N \boldsymbol \phi(\boldsymbol x)
$$
方差中的第一项表示数据中的噪声，第二项反映了与参数$\boldsymbol w$关联的不确定性。

### 3.3 等价核

把
$$
\boldsymbol m_N = \beta \boldsymbol S_N \boldsymbol \Phi^T \boldsymbol t
$$
代入

$$
y(\boldsymbol x, \boldsymbol w) = \boldsymbol w^T \boldsymbol \phi(\boldsymbol x)
$$
得到
$$
\begin{align}
y(\boldsymbol x, \boldsymbol m_N) &= \boldsymbol m^T_N \boldsymbol \phi(\boldsymbol x) 
\\&= \beta \boldsymbol \phi(\boldsymbol x)^T \boldsymbol S_N \boldsymbol \Phi^T \boldsymbol t
\\&= \sum^N_{n=1} \beta \boldsymbol \phi(\boldsymbol x)^T \boldsymbol S_N \boldsymbol \phi(\boldsymbol x_n) t_n
\end{align}
$$
因此在点$\boldsymbol x$处的预测均值由训练集⽬标变量$t_n$的线性组合给出，即
$$
y(\boldsymbol x, \boldsymbol m_N) = \sum^N_{n=1} k(\boldsymbol x, \boldsymbol x_n) t_n
$$
其中，函数
$$
k(\boldsymbol x, \boldsymbol x') = \boldsymbol \phi(\boldsymbol x)^T \boldsymbol S_N \boldsymbol \phi(\boldsymbol x')
$$
被称为**平滑矩阵** (smoother matrix) 或者**等价核** (equivalent kernel)



## 4. 贝叶斯模型比较

假设我们想比较$L$个模型$\{\mathcal M_i\}$，其中$i=1, \cdots, L$。一个模型指的是观测数据$\mathcal D$上的概率分布。

> 每一个“模型”$\mathcal M_i$都是一个数学构造，它定义了在该模型的假设下，数据$\mathcal D$出现的概率分布。

我们会假设数据是由这些模型中的一个生成的，但不知道是哪一个，不确定性通过先验概率分布$p(\mathcal M_i)$给出，可以表示不同模型的优先级。给定一个训练数据集$\mathcal D$，我们想估计后验分布
$$
p(\mathcal M_i|\mathcal D) \propto p(\mathcal M_i)p(\mathcal D|\mathcal M_i)
$$
其中$p(\mathcal D|\mathcal M_i)$被称为**模型证据** (model evidence) 或者**边缘似然** (marginal likelihood)。

一旦知道了模型上的后验概率分布，则预测分布为
$$
p(t|\boldsymbol x, \mathcal D) = \sum^L_{i=1}p(t|\boldsymbol x, \mathcal M_i, \mathcal D) p(\mathcal M_i|\mathcal D)
$$
这个公式中，整体的预测分布 = 各个模型的预测分布$p(t|\boldsymbol x, \mathcal M_i, \mathcal D)$求加权平均，权值为这些模型的后验概率$p(\mathcal M_i|\mathcal D)$。

### 4.1 模型证据

对于一个由参数$\boldsymbol w$控制的模型，模型证据为
$$
p(\mathcal D|\mathcal M_i) = \int p(\mathcal D|\boldsymbol w, \mathcal M_i) p(\boldsymbol w| \mathcal M_i) d\boldsymbol w
$$


## 5. 证据近似

引入$\alpha$和$\beta$上的超先验分布，则预测分布可以通过对$\boldsymbol w, \alpha, \beta$求积分的方式得到
$$
p(t|\mathbf t) = \int\int\int p(t|\boldsymbol w, \beta) p(\boldsymbol w|\mathbf t, \alpha, \beta) p(\alpha, \beta| \mathbf t) \; d\boldsymbol w \;d\alpha \;d\beta
$$

> 超先验分布：
> 在贝叶斯统计的层级模型中使用的概念。在层级贝叶斯模型中，我们不仅为模型参数指定先验分布，而且还为控制先验分布形状的参数，即超参数，也指定先验分布。这个为超参数指定的先验分布就被称为超先验分布。
> $$
> p(t|\boldsymbol w, \beta) = \mathcal N(t|y(\boldsymbol x, \boldsymbol w), \beta^{-1}) \\
> p(\boldsymbol w|\mathbf t, \alpha, \beta) = \mathcal N(\boldsymbol w|\boldsymbol m_N, \boldsymbol S_N)
> $$



### 5.1 计算证据函数

证据函数是通过对权值参数$\boldsymbol w$进行积分得到的，即
$$
p(\mathbf t|\alpha, \beta) = \int p(\mathbf t|\boldsymbol w, \beta)p(\boldsymbol w|\alpha) d\boldsymbol w
$$

> $$
> p(\mathbf t|\boldsymbol X, \boldsymbol w, \beta) = \prod^N_{n=1} \mathcal N(t_n|\boldsymbol w^T \boldsymbol \phi(\boldsymbol x_n), \beta^{-1}) \\
> p(\boldsymbol w| \alpha) = \mathcal N(\boldsymbol w| \boldsymbol 0, \alpha^{-1}\boldsymbol I)
> $$
>
> 
>

计算这个积分，可以通过对指数项配平方，然后使用高斯分布的归一化系数的基本形式
$$
p(\mathbf t| \alpha, \beta) = \left(\frac{\beta}{2\pi} \right)^{\frac{N}{2}} \left(\frac{\alpha}{2\pi} \right)^{\frac{M}{2}} \int \exp\{-E(\boldsymbol w) \}d \boldsymbol w 
$$
其中$M$是$\boldsymbol w$的维数，且
$$
E(\boldsymbol w) = \beta E_D(\boldsymbol w) + \alpha E_W(\boldsymbol w) = \frac{\beta}{2}||\mathbf t - \boldsymbol \Phi \boldsymbol w||^2 + \frac{\alpha}{2} \boldsymbol w^T \boldsymbol w
$$
可以发现上式等于正则化的平方和误差函数（忽略一些常数）。

对$\boldsymbol w$配平方，可得
$$
E(\boldsymbol w) = E(\boldsymbol m_N) + \frac{1}{2} (\boldsymbol w-\boldsymbol m_N)^T \boldsymbol A (\boldsymbol w-\boldsymbol m_N)
$$
其中
$$
\boldsymbol A = \alpha \boldsymbol I + \beta \boldsymbol \Phi^T \boldsymbol \Phi = \nabla\nabla E(\boldsymbol w) \\
E(\boldsymbol m_N) = \frac{\beta}{2} ||\mathbf t-\boldsymbol \Phi \boldsymbol m_N||^2 + \frac{\beta}{2}\boldsymbol m^T_N \boldsymbol m \\
\boldsymbol m_N = \beta \boldsymbol A^{-1}\boldsymbol \Phi^T \mathbf t
$$
注意$\boldsymbol A$对应于误差函数的二阶导数，被称为**Hessian矩阵**，且$\boldsymbol A = \boldsymbol S^{-1}_N$，因此$\boldsymbol m_N$表示后验概率分布的均值，$\boldsymbol A$是后验分布的精度。

于是关于$\boldsymbol w$的积分容易计算，因为
$$
\int\exp\left\{ \frac{1}{2} (\boldsymbol w-\boldsymbol m_N)^T \boldsymbol A (\boldsymbol w-\boldsymbol m_N) \right\} \boldsymbol w = (2\pi)^{\frac M 2}|\boldsymbol A|^{-\frac 1 2}
$$
因此，边缘似然函数的对数可写为
$$
\ln p(\mathbf t|\alpha, \beta) = \frac M 2 \ln\alpha + \frac N 2 \ln\beta - E(\boldsymbol m_N) - \frac 12\ln|\boldsymbol A| - \frac N2 \ln (2\pi)
$$










































