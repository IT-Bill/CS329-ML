## Question 1

$$
E_D (\mathbf{w}) = \frac{1}{2}\sum_{n=1}^Nr_n(t_n-\mathbf{w}^\text T\phi(\mathbf{x}_n))^2
$$

$$
\frac{\partial E_D (\mathbf{w})}{\partial \mathbf{w}} = - \sum_{n=1}^N r_n \phi(\mathbf{x}_n)^\text T (t_n-\mathbf{w}^\text T\phi(\mathbf{x}_n)) = 0
$$

It is solved that
$$
\mathbf{w}^* = \left( \sum_{n=1}^{N} r_n \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T \right)^{-1} \left( \sum_{n=1}^{N} r_n t_n \phi(\mathbf{x}_n) \right) 
$$

#### Data Dependent Noise Variance

The weighting factors $r_n$ can be considered as inversely proportional to the variance of the noise associated with each data point. Higher weights imply lower noise variance and thus more reliable data points. This perspective is useful in scenarios where different data points have varying levels of reliability or precision.

#### Replicated Data Points

each weight $r_n$ as representing the effective number of times that the data point $n$ is replicated in the dataset. A higher weight for a data point means it's as if that data point appears multiple times, giving it more influence on the model.



## Question 2

$$
\text{Gam}(\beta|a_0, b_0) = \frac{{b_0}^{a_0}}{\Gamma(a_0)} \beta^{a_0 - 1} \exp(-b_0 \beta)
$$

$$
p(\mathbf{w},\beta|\mathbf{t}) \propto p(\mathbf{t}|\mathbf{X},{\rm w},\beta)\;p(\mathbf{w},\beta)
$$

$$
p(\mathbf{t}|\mathbf{X},{\rm w},\beta) \propto \prod^N_{n=1} \beta^{\frac{1}{2}} \exp \left\{ -\frac{1}{2} (t_n - \mathbf{w}^\text T \Phi(x_n))^2\right\}
$$

$$
p(\mathbf{w},\beta) \propto \left( \frac{\beta}{|\mathbf{S}_0|} \right)^{\frac{1}{2}} \exp \left\{ -\frac{\beta}{2}(\mathbf w - \mathbf m_0)^\text T \mathbf S_0 (\mathbf w - \mathbf m_0) \right\} b_0 ^{a_0} \beta^{a_0 - 1} \exp(-b_0 \beta)
$$

Quadratic part
$$
\sum_{n=1}^N \frac{\beta}{2} \mathbf{w}^T \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T \mathbf{w} - \frac{\beta}{2} \mathbf{w}^T S_0^{-1} \mathbf{w} = \frac{\beta}{2} \mathbf{w}^T \left( \sum_{n=1}^N \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T + S_0^{-1} \right) \mathbf{w}
$$
Linear part
$$
\beta \mathbf{m}_0^T S_0^{-1} \mathbf{w} + \sum_{n=1}^N \beta t_n \phi(\mathbf{x}_n)^T \mathbf{w} = \beta \left( \mathbf{m}_0^T S_0^{-1} + \sum_{n=1}^N t_n \phi(\mathbf{x}_n)^T \right) \mathbf{w}
$$
Hence,
$$
S_N = \left( \sum_{n=1}^N \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T + S_0^{-1} \right)^{-1} \\
\mathbf{m}_N = S_N \left( S_0^{-1} \mathbf{m}_0 + \sum_{n=1}^N t_n \phi(\mathbf{x}_n) \right)
$$
Constant part
$$
\begin{align*}
p(\mathbf{w},\beta|\mathbf{t}) &\propto  -\frac{\beta}{2}\mathbf{m}_0^\top\mathbf{S}_0^{-1}\mathbf{m}_0 - b_0\beta - \frac{\beta}{2}\sum_{n=1}^{N}t_n^2
\\&= -\beta\left( \frac{1}{2}\mathbf{m}_0^\top\mathbf{S}_0^{-1}\mathbf{m}_0 + b_0 + \frac{1}{2}\sum_{n=1}^{N}t_n^2 \right)
\\&= -\beta\left( \frac{1}{2} \mathbf{m}_N^\top\mathbf{S}_N^{-1}\mathbf{m}_N + b_N \right) 
\end{align*}
$$

$$
\beta^{\frac{1}{2}}\beta^{a_0}\beta^{\frac{N}{2}} = \beta^{\frac{N}{2} + a_0 - \frac{1}{2}}
$$

$$
a_N = a_0 + \frac{N}{2} \\
b_N = \frac{1}{2}\mathbf{m}_0^\top\mathbf{S}_0^{-1}\mathbf{m}_0 + b_0 + \frac{1}{2}\sum_{n=1}^{N}t_n^2 - \frac{1}{2}\mathbf{m}_N^\top\mathbf{S}_N^{-1}\mathbf{m}_N
$$



## Question 3

$$
E(\mathbf w) = E(\mathbf m_N) + \frac{1}{2} (\mathbf w - \mathbf m_N) ^ \text T \mathbf A (\mathbf w - \mathbf m_N)
$$

$$
\begin{align*}
\int \exp( -E(w) ) dw &= \exp( -E(m_N) ) \int \exp \left( -\frac{1}{2} (w - m_N)^T A (w - m_N) \right) dw
\\ &= \exp( -E(m_N) )(2\pi)^{\frac{M}{2}} |A|^{-\frac{1}{2}}
\end{align*}
$$

Hence,
$$
 \ln p(t | \alpha, \beta) = \frac{M}{2} \ln \alpha + \frac{N}{2} \ln \beta - E(m_N) - \frac{1}{2} \ln |A| - \frac{N}{2} \ln(2\pi)
$$


## Question 4

$$
L(a) = \ln\prod^n_{i=1} p(Y_i|X_i, a) = -\frac{n}{2} ln(2\pi) - n\ln\sigma - \sum^n_{i=1} \frac{1}{2\sigma^2}(Y_i - aX_i)^2
$$

$$
\frac{\partial}{\partial a} L(a) = \frac{1}{\sigma^2} \sum^n_{i=1} X_i(Y_i - aX_i) = 0
$$

It is solved that
$$
a_{\text{ML}} = \frac{\sum^n_{i=1}X_i Y_i}{\sum^n_{i=1}X_i^2}
$$


## Question 5

$$
\begin{align*}
L(\theta) &= \ln \prod ^n_{i=1} \frac{\theta^{y_i} e^{-\theta}}{y_i!}
\\&= \sum^n_{i=1}y_i \ln\theta - n\theta - \sum^n_{i=1}\ln(y_i!)

\end{align*}
$$



## Question 6

$$
\begin{align*}
L(\lambda) &= \ln \prod^n_{i=1}\frac{1}{\Gamma(\alpha)}\lambda^{\alpha}X_i^{\alpha-1}e^{-\lambda X_i}
\\&= \ln \frac{1}{\Gamma(\alpha)^n} \lambda^{\alpha n} \left(\prod^n_{i=1}X_i^{\alpha-1}\right) e^{-\lambda \sum^n_{i=1}X_i}
\end{align*}
$$

$$
\frac{\partial}{\partial \lambda} L(\lambda) = \frac{\alpha n}{\lambda} - \sum^n_{i=1}X_i = 0
$$

It is solved that
$$
\lambda_{\text{ML}} = \frac{\alpha n}{\sum^n_{i=1}X_i}
$$











