## Question 1

$$
E_D (\mathbf{w}) = \frac{1}{2}\sum_{n=1}^Nr_n(t_n-\mathbf{w}^\text T\phi(\mathbf{x}_n))^2
$$

$$
\frac{\partial E_D (\mathbf{w})}{\partial \mathbf{w}} = - \sum_{n=1}^N r_n \phi(\mathbf{x}_n)^\text T (t_n-\mathbf{w}^\text T\phi(\mathbf{x}_n)) = 0
$$

It is solved that
$$
\mathbf{w}^* = \left( \sum_{n=1}^{N} r_n \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T \right)^{-1} \left( \sum_{n=1}^{N} r_n t_n \phi(\mathbf{x}_n) \right) 
$$

#### Data Dependent Noise Variance

The weighting factors $r_n$ can be considered as inversely proportional to the variance of the noise associated with each data point. Higher weights imply lower noise variance and thus more reliable data points. This perspective is useful in scenarios where different data points have varying levels of reliability or precision.

#### Replicated Data Points

each weight $r_n$ as representing the effective number of times that the data point $n$ is replicated in the dataset. A higher weight for a data point means it's as if that data point appears multiple times, giving it more influence on the model.



## Question 2

$$
\text{Gam}(\beta|a_0, b_0) = \frac{{b_0}^{a_0}}{\Gamma(a_0)} \beta^{a_0 - 1} \exp(-b_0 \beta)
$$

$$
p(\mathbf{w},\beta|\mathbf{t}) \propto p(\mathbf{t}|\mathbf{X},{\rm w},\beta)\;p(\mathbf{w},\beta)
$$

$$
p(\mathbf{t}|\mathbf{X},{\rm w},\beta) \propto \prod^N_{n=1} \beta^{\frac{1}{2}} \exp \left\{ -\frac{1}{2} (t_n - \mathbf{w}^\text T \Phi(x_n))^2\right\}
$$

$$
p(\mathbf{w},\beta) \propto \left( \frac{\beta}{|\mathbf{S}_0|} \right)^{\frac{1}{2}} \exp \left\{ -\frac{\beta}{2}(\mathbf w - \mathbf m_0)^\text T \mathbf S_0 (\mathbf w - \mathbf m_0) \right\} b_0 ^{a_0} \beta^{a_0 - 1} \exp(-b_0 \beta)
$$

Quadratic part
$$
\sum_{n=1}^N \frac{\beta}{2} \mathbf{w}^T \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T \mathbf{w} - \frac{\beta}{2} \mathbf{w}^T S_0^{-1} \mathbf{w} = \frac{\beta}{2} \mathbf{w}^T \left( \sum_{n=1}^N \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T + S_0^{-1} \right) \mathbf{w}
$$
Linear part
$$
\beta \mathbf{m}_0^T S_0^{-1} \mathbf{w} + \sum_{n=1}^N \beta t_n \phi(\mathbf{x}_n)^T \mathbf{w} = \beta \left( \mathbf{m}_0^T S_0^{-1} + \sum_{n=1}^N t_n \phi(\mathbf{x}_n)^T \right) \mathbf{w}
$$
Hence,
$$
S_N = \left( \sum_{n=1}^N \phi(\mathbf{x}_n) \phi(\mathbf{x}_n)^T + S_0^{-1} \right)^{-1} \\
\mathbf{m}_N = S_N \left( S_0^{-1} \mathbf{m}_0 + \sum_{n=1}^N t_n \phi(\mathbf{x}_n) \right)
$$
Constant part
$$
\begin{align*}
p(\mathbf{w},\beta|\mathbf{t}) &\propto  -\frac{\beta}{2}\mathbf{m}_0^\top\mathbf{S}_0^{-1}\mathbf{m}_0 - b_0\beta - \frac{\beta}{2}\sum_{n=1}^{N}t_n^2
\\&= -\beta\left( \frac{1}{2}\mathbf{m}_0^\top\mathbf{S}_0^{-1}\mathbf{m}_0 + b_0 + \frac{1}{2}\sum_{n=1}^{N}t_n^2 \right)
\\&= -\beta\left( \frac{1}{2} \mathbf{m}_N^\top\mathbf{S}_N^{-1}\mathbf{m}_N + b_N \right) 
\end{align*}
$$

$$
\beta^{\frac{1}{2}}\beta^{a_0}\beta^{\frac{N}{2}} = \beta^{\frac{N}{2} + a_0 - \frac{1}{2}}
$$

$$
a_N = a_0 + \frac{N}{2} \\
b_N = \frac{1}{2}\mathbf{m}_0^\top\mathbf{S}_0^{-1}\mathbf{m}_0 + b_0 + \frac{1}{2}\sum_{n=1}^{N}t_n^2 - \frac{1}{2}\mathbf{m}_N^\top\mathbf{S}_N^{-1}\mathbf{m}_N
$$



## Question 3

$$
E(\mathbf w) = E(\mathbf m_N) + \frac{1}{2} (\mathbf w - \mathbf m_N) ^ \text T \mathbf A (\mathbf w - \mathbf m_N)
$$

$$
\begin{align*}
\int \exp( -E(w) ) dw &= \exp( -E(m_N) ) \int \exp \left( -\frac{1}{2} (w - m_N)^T A (w - m_N) \right) dw
\\ &= \exp( -E(m_N) )(2\pi)^{\frac{M}{2}} |A|^{-\frac{1}{2}}
\end{align*}
$$

Hence,
$$
 \ln p(t | \alpha, \beta) = \frac{M}{2} \ln \alpha + \frac{N}{2} \ln \beta - E(m_N) - \frac{1}{2} \ln |A| - \frac{N}{2} \ln(2\pi)
$$


## Question 4

$$
L(a) = \ln\prod^n_{i=1} p(Y_i|X_i, a) = -\frac{n}{2} ln(2\pi) - n\ln\sigma - \sum^n_{i=1} \frac{1}{2\sigma^2}(Y_i - aX_i)^2
$$

$$
\frac{\partial}{\partial a} L(a) = \frac{1}{\sigma^2} \sum^n_{i=1} X_i(Y_i - aX_i) = 0
$$

It is solved that
$$
a_{\text{ML}} = \frac{\sum^n_{i=1}X_i Y_i}{\sum^n_{i=1}X_i^2}
$$


## Question 5

$$
\begin{align*}
L(\theta) &= \ln \prod ^n_{i=1} \frac{\theta^{y_i} e^{-\theta}}{y_i!}
\\&= \sum^n_{i=1}y_i \ln\theta - n\theta - \sum^n_{i=1}\ln(y_i!)

\end{align*}
$$



## Question 6

$$
\begin{align*}
L(\lambda) &= \ln \prod^n_{i=1}\frac{1}{\Gamma(\alpha)}\lambda^{\alpha}X_i^{\alpha-1}e^{-\lambda X_i}
\\&= \ln \frac{1}{\Gamma(\alpha)^n} \lambda^{\alpha n} \left(\prod^n_{i=1}X_i^{\alpha-1}\right) e^{-\lambda \sum^n_{i=1}X_i}
\end{align*}
$$

$$
\frac{\partial}{\partial \lambda} L(\lambda) = \frac{\alpha n}{\lambda} - \sum^n_{i=1}X_i = 0
$$

It is solved that
$$
\lambda_{\text{ML}} = \frac{\alpha n}{\sum^n_{i=1}X_i}
$$





这些问题都是关于期望最大化（EM）算法的，这是一种在统计学和机器学习中用于参数估计的迭代方法。下面是对每个问题的简要而严谨的回答：

### 1. EM算法的过程以及在机器学习中的应用

EM算法用于含有隐变量（latent variables）的统计模型的最大似然估计。当模型的似然函数因为存在隐变量而难以直接最大化时，EM算法提供了一种有效的迭代方法来估计模型参数。

EM算法分为两个步骤：期望步骤（E步）和最大化步骤（M步）。
- **E步**: 估计隐变量的分布，即计算隐变量的期望。
- **M步**: 最大化E步中得到的期望，以更新模型参数。

例如，在混合高斯模型中，观测数据是由多个高斯分布混合生成的，每个高斯分布对应一个隐变量，代表了数据点属于该分布的概率。EM算法可以用来估计每个高斯分布的均值和协方差以及混合比例。

### 2. EM算法和Q函数

Q函数是在E步骤中计算的，它代表了给定当前参数估计下隐变量分布的对数似然的期望。Q函数的定义为：

\[ 
$$
Q(\theta, \theta^{(t)}) = E_{Z|X,\theta^{(t)}}[\log p(X,Z|\theta)]
$$
 \]

其中 $ \theta^{(t)} $ 是当前迭代下的参数估计，而 $ \theta $ 是新的参数值。E步涉及计算 $ Q(\theta, \theta^{(t)}) $，M步涉及找到使得 $ Q(\theta, \theta^{(t)}) $ 最大化的 $ \theta $。

### 3. EM算法、似然和KL散度

EM算法可以视为最大化似然和最小化KL散度的过程。在EM算法的上下文中，KL散度衡量了当前参数下隐变量真实分布和估计分布之间的差异。

\[ 
$$
\text{KL}(p(Z|X, \theta^{(t)}) \| p(Z|X, \theta)) = -\int p(Z|X, \theta^{(t)}) \log \frac{p(Z|X, \theta)}{p(Z|X, \theta^{(t)})} dz
$$
 \]

在E步，我们寻找一个新的隐变量分布来减少这个KL散度，而在M步，我们更新参数 $ \theta $ 来最大化似然。

### 4. EM算法和非凸函数优化

EM算法是非凸优化问题中的一种方法，可以找到局部最优解。在每个M步，我们找到使得似然函数局部最大化的参数值。这个过程可以用非凸函数的优化曲线来可视化，其中EM迭代在概率分布空间中“攀爬”以寻找一个局部最高点。

### 5. EM算法和因子图网络模型

在因子图网络模型中，EM算法用于计算图模型中隐变量的概率分布。因子图是一种表示概率分布分解的图形模型。在这种情况下，E步涉及到使用消息传递算法（如置信传播）来推断隐变量的分布，M步则涉及到使用这些推断来更新模型参数。

例如，在语音识别中，因子图可以用来表示语音信号的统计模型，隐变量可能表示语言中的词汇或语音单元，EM算法用来估计这些隐变量的概率分布以及与之相关的模型参数。



这些问题都是关于强化学习的，这是机器学习的一个子领域，专注于如何让智能体在环境中采取行动以最大化某种累积奖励。以下是对每个问题的简要而严谨的回答：

### 1. Bellman方程及其求解方法

Bellman方程是强化学习中的一个基本方程，它提供了一种递归的方式来表达最优策略下的价值函数。对于状态价值函数 $V^*(s)$，Bellman最优方程为：

\[ 
$$
V^*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma V^*(s')]
$$
 \]

其中 $ \gamma $ 是折扣因子，$ p(s', r | s, a) $ 是从状态 $ s $ 通过行动 $ a $ 转移到状态 $ s' $ 并获得奖励 $ r $ 的概率。

求解Bellman方程通常涉及迭代方法，如价值迭代或策略迭代，不断更新价值函数直至收敛。

### 2. 策略迭代与价值迭代的区别及其优劣

策略迭代和价值迭代是求解Bellman方程的两种常见方法。

- **策略迭代**包括两个步骤：策略评估（计算当前策略下的价值函数）和策略改进（基于价值函数改进策略）。它的优点是通常比价值迭代更快收敛，但每次迭代都需要求解整个MDP，这可能计算上更昂贵。
  
- **价值迭代**直接迭代更新价值函数，直到收敛，然后从最终的价值函数导出策略。它的优点是实现简单，计算上通常比策略迭代更省资源，但可能需要更多的迭代次数才能收敛。

### 3. 无模型强化学习

无模型强化学习指的是智能体不需要学习或知道环境的模型（即转移概率和奖励函数），而是直接从与环境的交互中学习策略。这可以通过方法如Q学习或Sarsa来实现，这些方法通过采样的方式直接估计价值函数或动作价值函数。

### 4. 在线与离线强化学习的区别

- **在线强化学习**是指智能体在学习的同时与环境交互。这意味着智能体的每一步行动都是基于当前学到的知识，这可以加速学习但也可能导致在学习过程中做出不好的决策。
  
- **离线强化学习**是指智能体从已经收集好的数据（例如通过早期的在线交互）中学习，不直接与环境交互。这允许智能体从大量数据中学习，但可能导致策略过于依赖于数据集中的分布。

### 5. 在策和离策强化学习的区别

- **在策（On-policy）强化学习**是指智能体学习和评估基于当前策略采取的行动。Sarsa是一个典型的在策学习算法。
  
- **离策（Off-policy）强化学习**是指智能体学习一个与它用来探索环境的策略不同的策略。Q学习是一个典型的离策学习算法。

在策学习的优点是策略始终与智能体的行为保持一致，而离策学习的优点是智能体可以学习一个最优策略，而不必将其用于探索。但离策算法可能更难收敛，因为它涉及到从可能与目标策略有很大不同的行为策略中学习。
